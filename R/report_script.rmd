---
title: "Structural Equation Modeling"
author: "Julian Ibarguen"
date: "25/08/2022"
header-includes:
- \usepackage{booktabs}
- \usepackage{makecell}
- \usepackage{colortbl}
- \usepackage{wrapfig}
- \usepackage{subfig}
- \usepackage{lscape}
- \usepackage{natbib}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
    extra_dependencies: "subfig"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)


format_tbl <- function(.data, caption = "Table") {
  
  align.c <- rep("c", ncol(.data) - 1)
  
  kableExtra::kbl(
    .data, format = "latex", booktabs = TRUE,
    align = c("l", align.c), digits = 3,
    caption = caption
  ) %>%
    kableExtra::kable_styling(latex_options = c("hold_position", "striped")) %>%
    kableExtra::row_spec(0, bold = TRUE) %>%
    kableExtra::column_spec(0, bold = TRUE)

}

```

## Introduction

We aim to fit a Structural Equation Model (SEM) following the exercise proposed by Petri Nokelainen from the Research Centre for Vocational Education, University of Tampere, Finland. Data and documentation for this exercise can be found [\textcolor{blue}{here}](https://www.scribd.com/document/222003310/Sem-Exercise-v2-5).

We used the proposed data set with `r nrow(df)` rows and `r ncol(df)` columns. The respondents in both samples are staff members of Finnish polytechnic institute for higher education. The original measurement instrument (Growth-oriented Atmosphere Questionnaire, GOAQ) has 13 factors and 92-items (Ruohotie, 1996; Ruohotie, Nokelainen & Tirri, 2002), but for the purposes of this exercise Nokelainen selected 13 factors and 27 items, all consisting of a fiveâ€“point Likert scale from 1 (totally disagree) to 5 (totally agree) was applied (Nokelainen, n.d.)

## Evaluation of assumptions

### Univariate distribution

Although data is ordinal, thus non-normal in nature, we check for normality assumption. We will apply z re-scaling to see how they approximate to mean 0. Will also check kurtosis and skewnewss, and plot the histograms. We reject applying Saphiro test or other normality test, as being only 5 points scale ordinal is unlikely will provide any meaningful result.

Given a normally distribute variable would have a skewness of 0 and kurtosis of 3. For this purpose we calculated a synthetic measure for skewness and kurtosis as the average between the skewness and excess kurtosis ($kurtosis - 3$). Under this synthetic measure, a perfect normal distribution would have a value of 0. For our purpose, we select those variables whose deviance from the normal distribution is lower than 0.4, according to the synthetic measure create. The selected variables were further confirmed with the histogram.


Observing table 1 and figure 1, we assumed normality for the following variables: `r paste(norm_stats[norm_stats$avg_skew_kurt < 0.4, "variable"][[1]], collapse = ", ")`. The remaining variables were assumed non-normally distributed


```{r norm_descr}
format_tbl(norm_stats, "Descriptive statistics to assess approximation to normal distribution")
```


```{r histogram, fig.dim = c(11,15), fig.cap = "Histograms"}
p_hist
```

### Bivariate distribution

Based on the univariate analysis, we select three different variable combinations: 1) both are assumed normally distributed; 2) one is assumed normally distributed, but the other one no; 3) neither fo them is assumed normally distributed. 

In figure 2, we can observe the linear relationship between the 3 different combination. When we assume normality in both variables, we observe linear relationship, which decreases for the second combination (one variable assumed normal and the other no), and the relationship disappear for the third combination

```{r bivariates, fig.show = "hold", fig.ncol = 2, fig.align = 'center', out.width = "50%", fig.cap = "Bivariate distributions", fig.subcap = c("Two variables assumed normally distributed", "One variable assumed normally distributed", "No variable assumed normally distributed")}
p_norm2
p_norm1
p_norm0
```


### Correlation matrix

We observe now the correlation matrix between all `r ncol(df)` variables. For SEM and other factorial type of analysis, the correlation coefficient should be between $\pm 0.3$ and $\pm 0.8$. Too low correlations indicate weak inter-item dependency, too high correlations might indicate multicollinearity (Nokelainen, n.d.). 

The highest correlation coefficient ($r$) in absolute terms was betwee `r max_r_vars` with $r = $`r max_r` and $R^2$ = `r `max_r2`. The lowest correlation coefficient ($r$) was between `r min_r_vars` with $r = $`r min_r` and $R^2$ = `r `min_r2`. Therefore, we cna conlcude that our variables are suitable for a SEM model


```{r cor_plot, fig.dim = c(15, 15), fig.cap = "Correlation matrix"}
corrplot.mixed(
  cor(df, use = "complete.obs"), 
  lower = 'number', upper = 'square', 
  tl.cex = 1.25, number.cex = 1, number.digits = 2
)
```


## Path analysis

To perform the path analysis we make use of the 13 factors available in our data and we fit the following model:

```{r path_plot, out.width = "60%", fig.cap = "Model path", fig.align = 'center'}
p_path
```

where:

```{r var_names}
var_dict <- as_tibble(as.list(var_labs)) %>% 
  tidyr::pivot_longer(1:length(var_labs), names_to = "variable_name", values_to = "variable_label")

# var_dict[var_dict$variable_name %in% names(p_path$Arguments$labels) , "model_acronym"] <- p_path$Arguments$labels
# var_dict[is.na(var_dict$model_acronym),"model_acronym"] <- ""

var_dict <- dplyr::relocate(var_dict, variable_label, .before = 1)

format_tbl(var_dict, "Labels for factor variables")

```

Tghe summary of the model was the following:

```{r summary_fit}
fit_summary

```

```{r calculate}
r2 <- fit_pe[which(fit_pe$op == "r2"), "est"]

fit_pe_reg <- dplyr::arrange(fit_pe[fit_pe$op == "~", ], dplyr::desc(est))


r_1_var <- var_labs[names(var_labs) %in% fit_pe_reg[1, "rhs"]]


```

From the results of the model we obtain the following conclusions:

* How much dependent variable variance the four independent variables predict? $R^2 = `r round(r2,3)`$ .

* Order the IVs in the following rows (best predictor comes first):
  1. The first (strongest) predictor for Valuation of the work is _`r var_labs[names(var_labs) %in% fit_pe_reg[1, "rhs"]]`_ $r = `r round(fit_pe_reg[1, "est"], 3)`$
  2. The second predictor for Valuation of the work is _`r var_labs[names(var_labs) %in% fit_pe_reg[2, "rhs"]]`_ $r = `r round(fit_pe_reg[2, "est"], 3)`$
  3. The third predictor for Valuation of the work is _`r var_labs[names(var_labs) %in% fit_pe_reg[3, "rhs"]]`_ $r = `r round(fit_pe_reg[3, "est"], 3)`$
  4. The fourth predictor for Valuation of the work is _`r var_labs[names(var_labs) %in% fit_pe_reg[4, "rhs"]]`_ $r = `r round(fit_pe_reg[4, "est"], 3)`$

* Select Unstandardized estimates and complete the following sentences:
  * When _Encouraging leadership_ goes up by 1, Valuation of the work goes up / down by ____.
  * When _Know-how rewarding_ goes up by 1, Valuation of the work goes up / down by ____.




